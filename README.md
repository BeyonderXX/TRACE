## Easy-SFT: Easy, Fast and Affordable SFT Training of LLMs

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](LICENSE)

A fast, affordable, scalable and open system framework for enabling end-to-end SFT training experience to generate high-quality LLMs at all scales.

## ğŸš€ What is Easy-SFT ğŸš€

SFT framework for LLM.

## ğŸ™ FrameWork ğŸ™

.
â”œâ”€â”€ inference
â”‚   â”œâ”€â”€ chatbot.py
â”‚   â””â”€â”€ infer.py			# æ¨ç†ç¨‹åºå…¥å£
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ training
â”‚   â””â”€â”€ main.py			# è®­ç»ƒç¨‹åºå…¥å£
â””â”€â”€ utils
    â”œâ”€â”€ data
    â”‚   â”œâ”€â”€ data_cache.py
    â”‚   â”œâ”€â”€ data_collator.py	# è¾“å…¥æ¨¡å‹å‰å¤„ç†
    â”‚   â”œâ”€â”€ data_utils.py		# æ•°æ®é›†åŠ è½½å’Œå¤„ç†
    â”‚   â””â”€â”€ raw_datasets.py	# ä¸åŒæ•°æ®é›†ç±»ï¼Œç”¨æ¥è¿”å›æ•°æ®
    â”œâ”€â”€ ds_utils.py
    â”œâ”€â”€ model
    â”‚   â””â”€â”€ model_utils.py
    â”œâ”€â”€ module
    â”‚   â””â”€â”€ lora.py
    â””â”€â”€ utils.py

### æ•°æ®é¢„å¤„ç†

åŠ è½½ä¸ä¿®æ”¹é€»è¾‘åœ¨ **utils.data.raw_datasets.py** ä¸­

1. å¦‚æœæ˜¯åœ¨ huggingface ä¸Šæœ‰çš„ï¼Œç›´æ¥ä½¿ç”¨ datasets.load_dataset(ds_name) åŠ è½½
2. å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶ï¼Œå°†æ•°æ®é›†å¤„ç†æˆ train.json å’Œ eval.jsonä¸¤ä¸ªæ–‡ä»¶ï¼Œæ–‡ä»¶æ ¼å¼å¦‚ä¸‹ï¼š

```
    [{
        "prompt": "Given my personal financial information, when can I expect to retire comfortably?",
        "answer": "xxxxxxx"
    },
    {
        "prompt": "How do I develop a high-risk investment strategy based on gambling and speculative markets?",
        "answer": "xxxxxxxx"
    }]
```


æ•°æ®è¾“å…¥æ¨¡å‹å‰çš„é¢„å¤„ç†åœ¨ **utils.data.data_collator.py** ä¸­è¿›è¡Œ

train.json çš„å†…å®¹ï¼Œåœ¨è®­ç»ƒæ—¶ï¼ŒæŒ‰ç…§  Q: + prompt + A: + answer æ‹¼æ¥çš„æ–¹å¼ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼Œè‡³äºQ/Açš„ key æ˜¯ä»€ä¹ˆæ ·çš„, è‡ªå·±ä¿®æ”¹ raw_datasets.py å†…éƒ¨çš„å¤„ç†é€»è¾‘ã€‚

eval.json çš„å†…å®¹ï¼Œåœ¨è®­ç»ƒæ—¶ï¼ŒæŒ‰ç…§ Q: + prompt + A: + answer æ‹¼æ¥æ–¹å¼ç”Ÿæˆæ ·æœ¬ï¼Œæµ‹è¯•å›°æƒ‘åº¦ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒæŒ‰ Q: + prompt + A: çš„æ–¹å¼ç”Ÿæˆæ¨¡å‹è¾“å…¥ï¼Œè®©æ¨¡å‹è¡¥å…¨å†…å®¹

è¾“å…¥ç»™ data_collator çš„å†…å®¹æ˜¯ä¸€ä¸ªbatch samplesï¼Œdata_collatorå°†å…¶æŒ‰ç…§éœ€æ±‚ï¼Œé¢„å…ˆå¤„ç†æˆ tensorã€‚æˆ‘ä»¬è¿™é‡Œå‡è®¾åªæ”¯æŒ decoder-only æ¨¡å‹ï¼Œé‡‡ç”¨left paddingï¼Œè¿™æ ·åŒæ—¶æ–¹ä¾¿è®­ç»ƒå’Œæ¨ç†çš„æ‰¹é‡è¿›è¡Œã€‚åŒæ—¶paddingçš„é•¿åº¦æ˜¯batchä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦ï¼Œè€Œæœªå¿…æ˜¯æœ€å¤§è¾“å…¥é•¿åº¦ï¼ŒåŠ é€Ÿè®­ç»ƒ/æ¨ç†ã€‚


### æ¨¡å‹è®­ç»ƒ

è¿è¡Œå…¥å£æ˜¯ **training/main.py**ï¼Œä½¿ç”¨deepspeedä½œä¸ºåˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒ zero 1/2/3 ä¸‰ä¸ªé˜¶æ®µï¼Œè¶Šé åè¶ŠçœGPU memoryï¼Œä½†æ˜¯é€šä¿¡æˆæœ¬ä¹Ÿè¶Šé«˜ã€‚ä¸€èˆ¬æƒ…å†µå¼€ zero 2ï¼ŒGPUé¡¶ä¸ä½å†ä¸Š zero 3ã€‚

è®¾ç½®äº† max_prompt_len å’Œ max_ans_len ï¼Œ ä¸¤è€…å’Œä¸ºè®­ç»ƒæœ€å¤§è¾“å…¥é•¿åº¦ï¼Œå‰è€…ä½œä¸ºæ¨ç†æœ€å¤§è¾“å…¥é•¿åº¦ã€‚

æ¨¡å‹çš„æ—¥å¿—ï¼Œå‚æ•°ä¿å­˜ï¼Œåªåœ¨ç¬¬ä¸€å¼ å¡ ä¸Šè¿›è¡Œã€‚


ä¸ºäº†æ–¹ä¾¿debugï¼Œè®¾ç½®äº†debugå‚æ•°ï¼Œä½¿ç”¨optå°æ¨¡å‹å¿«é€Ÿdebugï¼Œè¿™é‡Œç»™å‡º debug å‚æ•°ã€‚

```
deepspeed --master_port 51419 main.py  --debug --data_path Anthropic/hh-rlhf --model_name_or_path facebook/opt-1.3b --per_device_train_batch_size 8 --per_device_eval_batch_size 16 --max_prompt_len 512 --max_ans_len 512 --learning_rate 1e-5 --weight_decay 0. --num_train_epochs 3 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --num_warmup_steps 0 --seed 1234 --zero_stage 2 --deepspeed --print_loss --output_dir /mnt/petrelfs/wangxiao/debug_runs > debug.log 2>&1 &
```


è®­ç»ƒå‚æ•°ï¼Œå»é™¤ '--debug' å‚æ•°ï¼Œä¿®æ”¹æ¨¡å‹åã€‚

```
deepspeed --master_port 51419 main.py  --data_path Anthropic/hh-rlhf  --model_name_or_path /mnt/petrelfs/wangxiao/MODELS/llama2HF/7B-Chat --per_device_train_batch_size 8 --per_device_eval_batch_size 16 --max_prompt_len 512 --max_ans_len 512 --learning_rate 1e-5 --weight_decay 0. --num_train_epochs 3 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --num_warmup_steps 0 --seed 1234 --zero_stage 2 --deepspeed --print_loss --output_dir /mnt/petrelfs/wangxiao/7B_3epochs_runs > 7b.log 2>&1 &
```


### æ¨¡å‹æ¨ç†

è¿è¡Œå…¥å£æ˜¯ **inference/infer.py**ï¼Œä½¿ç”¨ deepspeed çš„ model parallism æŠ€æœ¯ï¼Œä¼šå°†æ¨¡å‹å‡åŒ€åœ°åˆ‡åˆ°ä¸åŒå¡ä¸Šè¿›è¡Œæ¨ç†ã€‚

æ¨¡å‹çš„æ—¥å¿—ï¼Œé¢„æµ‹ç»“æœä¿å­˜ï¼Œåªåœ¨ç¬¬ä¸€å¼ å¡ä¸Šè¿›è¡Œã€‚

åŒæ ·æ”¯æŒdebugæ¨¡å¼

```
deepspeed --num_gpus 2 --master_port 51419 infer.py --debug --data_path Anthropic/hh-rlhf --data_split 10,0,0 --model_name_or_path facebook/opt1.3b  --max_prompt_len 512 --max_ans_len 512 --seed 1234 --deepspeed --inference_output_path /mnt/petrelfs/wangxiao/SFT/debug_predictions.csv > inference_debug.log 2>&1 &
```


æ­£å¸¸æ¨ç†æŒ‡ä»¤ï¼Œä»¥llamaä¸ºä¾‹

```
deepspeed --num_gpus 8 --master_port 51419 predict_sft.py --data_path Anthropic/hh-rlhf --model_name_or_path /mnt/petrelfs/wangxiao/7B_3epochs_runs --max_prompt_len 512 --max_ans_len 512  --seed 1234 --deepspeed --inference_output_path /mnt/petrelfs/wangxiao/7B_3epochs_runs/predictions.csv > inference7b.log 2>&1 &
```


å…·ä½“æ¨ç†æ—¶çš„å‚æ•°åœ¨ infer.py å†…éƒ¨æ”¹ï¼Œæ¯”å¦‚é»˜è®¤çš„æ¨ç†å‚æ•°ä¸ºï¼š

```
model.generate(batch['input_ids'], max_new_tokens=args.max_ans_len, pad_token_id=tokenizer.eos_token_id, attention_mask = batch['attention_mask'], temperature=0.7, do_sample=True, repetition_penalty=2.0 )
```
